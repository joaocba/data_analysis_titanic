import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import learning_curve
from sklearn.preprocessing import LabelEncoder


def show(data):
    def ensure_dummies(data):
        if 'Embarked_C' not in data.columns or 'Embarked_Q' not in data.columns or 'Embarked_S' not in data.columns:
            data = pd.get_dummies(data, columns=['Embarked'], prefix='Embarked')
        return data

    # Criar c√≥pia dos dados para n√£o modificar o original
    data = data.copy()

    # Converter 'Survived' para valores num√©ricos
    le = LabelEncoder()
    if 'Survived' in data.columns:
        if data['Survived'].dtype == 'object':
            data['Survived'] = le.fit_transform(data['Survived'])

    # Garantir que as vari√°veis dummy est√£o presentes
    data = ensure_dummies(data)

    # Converter 'Sex' para num√©rico se necess√°rio
    if 'Sex' in data.columns and data['Sex'].dtype == 'object':
        data['Sex'] = le.fit_transform(data['Sex'])

    # Definir as caracter√≠sticas para o modelo
    features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_C', 'Embarked_Q', 'Embarked_S']

    # Verificar caracter√≠sticas
    missing_features = [feature for feature in features if feature not in data.columns]
    if missing_features:
        st.error(f"Colunas em falta no conjunto de dados: {', '.join(missing_features)}")
        return

    # Preparar dados
    X = data[features]
    y = data['Survived']
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Cabe√ßalho com t√≠tulo e descri√ß√£o
    st.title("ü§ñ Modela√ß√£o Preditiva")

    st.markdown("""
    Esta an√°lise utiliza dois modelos de aprendizagem autom√°tica diferentes para prever a sobreviv√™ncia 
    dos passageiros do Titanic. Compare os resultados e explore as conclus√µes geradas por cada modelo.

    #### üéØ Objectivos:
    - Avaliar o desempenho de diferentes modelos
    - Identificar os factores mais importantes para sobreviv√™ncia
    - Comparar m√©tricas e resultados
    """)

    # Criar separadores principais com √≠cones e descri√ß√µes
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìä Vis√£o Geral dos Dados",
        "üå≥ √Årvore de Decis√£o",
        "üéØ KNN",
        "üìà Compara√ß√£o e Conclus√µes"
    ])

    # Separador 1: Vis√£o Geral dos Dados
    with tab1:
        st.header("üìä Vis√£o Geral dos Dados de Treino e Teste")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric(
                label="üìù Total de Amostras",
                value=len(X),
                delta=f"{len(X_train)} treino / {len(X_test)} teste"
            )

        with col2:
            survival_rate = (y == 1).mean() * 100
            st.metric(
                label="üí´ Taxa de Sobreviv√™ncia",
                value=f"{survival_rate:.1f}%",
                delta=f"{len(features)} caracter√≠sticas utilizadas"
            )

        with col3:
            class_balance = f"{(y_train == 1).mean() * 100:.1f}% / {(y_test == 1).mean() * 100:.1f}%"
            st.metric(
                label="‚öñÔ∏è Equil√≠brio das Classes",
                value=class_balance
            )

        st.subheader("üîç Caracter√≠sticas Utilizadas no Modelo")

        feature_df = pd.DataFrame({
            'Caracter√≠stica': features,
            'Tipo': [X[f].dtype for f in features],
            'Valores √önicos': [X[f].nunique() for f in features],
            'Valores em Falta': [X[f].isnull().sum() for f in features]
        })

        st.dataframe(
            feature_df.style.background_gradient(subset=['Valores √önicos'], cmap='YlOrRd')
            .background_gradient(subset=['Valores em Falta'], cmap='RdYlGn_r'),
            hide_index=True
        )

# Separador 2: √Årvore de Decis√£o
    with tab2:
        st.header("üå≥ Modelo: √Årvore de Decis√£o")

        # Par√¢metros ajust√°veis na mesma p√°gina
        col1, col2 = st.columns(2)
        with col1:
            max_depth = st.slider("Profundidade M√°xima", 1, 20, 5)
        with col2:
            min_samples_split = st.slider("M√≠nimo para Divis√£o", 2, 20, 2)

        # Treino com par√¢metros ajust√°veis
        dt_model = DecisionTreeClassifier(
            random_state=42,
            max_depth=max_depth,
            min_samples_split=min_samples_split
        )
        dt_model.fit(X_train, y_train)
        dt_pred = dt_model.predict(X_test)

        # M√©tricas
        col1, col2 = st.columns([1, 1])

        with col1:
            st.subheader("üìä M√©tricas de Desempenho")
            dt_accuracy = accuracy_score(y_test, dt_pred)
            dt_f1 = f1_score(y_test, dt_pred)

            metrics_col1, metrics_col2 = st.columns(2)
            with metrics_col1:
                st.metric("üéØ Precis√£o", f"{dt_accuracy:.3f}")
            with metrics_col2:
                st.metric("üìà Pontua√ß√£o F1", f"{dt_f1:.3f}")

        with col2:
            st.subheader("üîÑ Valida√ß√£o Cruzada")
            cv_scores = cross_val_score(dt_model, X, y, cv=5)
            st.metric("üîÑ M√©dia VC", f"{cv_scores.mean():.3f}")
            st.metric("üìä Desvio Padr√£o VC", f"{cv_scores.std():.3f}")

        # Matriz de Confus√£o
        st.subheader("üìä Matriz de Confus√£o")
        dt_conf_matrix = confusion_matrix(y_test, dt_pred)
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(dt_conf_matrix, annot=True, fmt='d', cmap='RdYlBu_r',
                    xticklabels=['N√£o Sobreviveu', 'Sobreviveu'],
                    yticklabels=['N√£o Sobreviveu', 'Sobreviveu'])
        plt.title('Matriz de Confus√£o - √Årvore de Decis√£o', pad=20)
        st.pyplot(fig)

        # Import√¢ncia das Caracter√≠sticas
        st.subheader("üîç Import√¢ncia das Caracter√≠sticas")
        feature_importance = pd.DataFrame({
            'Caracter√≠stica': features,
            'Import√¢ncia': dt_model.feature_importances_
        }).sort_values('Import√¢ncia', ascending=True)

        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.barh(feature_importance['Caracter√≠stica'], feature_importance['Import√¢ncia'])
        ax.set_title('Import√¢ncia das Caracter√≠sticas na √Årvore de Decis√£o')

        for i, v in enumerate(feature_importance['Import√¢ncia']):
            ax.text(v, i, f'{v:.3f}', va='center')

        sm = plt.cm.ScalarMappable(cmap='viridis',
                                   norm=plt.Normalize(0, max(feature_importance['Import√¢ncia'])))
        for bar, importance in zip(bars, feature_importance['Import√¢ncia']):
            bar.set_color(sm.to_rgba(importance))

        plt.tight_layout()
        st.pyplot(fig)

    # Separador 3: KNN
    with tab3:
        st.header("üéØ Modelo: K-Vizinhos Mais Pr√≥ximos (KNN)")

        # Par√¢metros ajust√°veis na mesma p√°gina
        col1, col2 = st.columns(2)
        with col1:
            n_neighbors = st.slider("N√∫mero de Vizinhos (K)", 1, 20, 5)
        with col2:
            weights = st.selectbox("Peso dos Vizinhos", ['uniforme', 'dist√¢ncia'])
            # Mapear op√ß√µes em portugu√™s para valores em ingl√™s
            weights_map = {'uniforme': 'uniform', 'dist√¢ncia': 'distance'}
            weights = weights_map[weights]

        # Treino com par√¢metros ajust√°veis
        knn_model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)
        knn_model.fit(X_train, y_train)
        knn_pred = knn_model.predict(X_test)

        # M√©tricas
        col1, col2 = st.columns([1, 1])

        with col1:
            st.subheader("üìä M√©tricas de Desempenho")
            knn_accuracy = accuracy_score(y_test, knn_pred)
            knn_f1 = f1_score(y_test, knn_pred)

            metrics_col1, metrics_col2 = st.columns(2)
            with metrics_col1:
                st.metric("üéØ Precis√£o", f"{knn_accuracy:.3f}")
            with metrics_col2:
                st.metric("üìà Pontua√ß√£o F1", f"{knn_f1:.3f}")

        with col2:
            st.subheader("üîÑ Valida√ß√£o Cruzada")
            cv_scores = cross_val_score(knn_model, X, y, cv=5)
            st.metric("üîÑ M√©dia VC", f"{cv_scores.mean():.3f}")
            st.metric("üìä Desvio Padr√£o VC", f"{cv_scores.std():.3f}")

        # Matriz de Confus√£o
        st.subheader("üìä Matriz de Confus√£o")
        knn_conf_matrix = confusion_matrix(y_test, knn_pred)
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(knn_conf_matrix, annot=True, fmt='d', cmap='RdYlBu_r',
                    xticklabels=['N√£o Sobreviveu', 'Sobreviveu'],
                    yticklabels=['N√£o Sobreviveu', 'Sobreviveu'])
        plt.title('Matriz de Confus√£o - KNN', pad=20)
        st.pyplot(fig)

    # Separador 4: Compara√ß√£o e Conclus√µes
    with tab4:
        st.header("üìà Compara√ß√£o dos Modelos e Conclus√µes")

        # Compara√ß√£o visual das m√©tricas
        comparison_df = pd.DataFrame({
            'Modelo': ['√Årvore de Decis√£o', 'KNN'],
            'Precis√£o': [dt_accuracy, knn_accuracy],
            'Pontua√ß√£o F1': [dt_f1, knn_f1]
        })

        # Gr√°fico de compara√ß√£o
        fig, ax = plt.subplots(figsize=(10, 6))
        x = np.arange(len(comparison_df['Modelo']))
        width = 0.35

        bars1 = ax.bar(x - width / 2, comparison_df['Precis√£o'], width, label='Precis√£o',
                       color='skyblue')
        bars2 = ax.bar(x + width / 2, comparison_df['Pontua√ß√£o F1'], width, label='Pontua√ß√£o F1',
                       color='lightcoral')

        ax.set_ylabel('Pontua√ß√£o')
        ax.set_title('Compara√ß√£o de M√©tricas entre Modelos')
        ax.set_xticks(x)
        ax.set_xticklabels(comparison_df['Modelo'])
        ax.legend()

        def autolabel(bars):
            for bar in bars:
                height = bar.get_height()
                ax.annotate(f'{height:.3f}',
                            xy=(bar.get_x() + bar.get_width() / 2, height),
                            xytext=(0, 3),
                            textcoords="offset points",
                            ha='center', va='bottom')

        autolabel(bars1)
        autolabel(bars2)

        plt.tight_layout()
        st.pyplot(fig)

        # Conclus√µes em markdown
        st.markdown("""
        #### üéØ Desempenho Global
        - A √Årvore de Decis√£o alcan√ßou uma precis√£o de {:.1f}% vs {:.1f}% do KNN
        - A Pontua√ß√£o F1 mostra uma diferen√ßa de {:.1f} pontos entre os modelos
        - A estabilidade da √Årvore de Decis√£o √© {}
        """.format(
            dt_accuracy * 100,
            knn_accuracy * 100,
            abs(dt_f1 - knn_f1) * 100,
            "superior" if dt_accuracy > knn_accuracy else "inferior"
        ))

        st.markdown("""
        #### üîç An√°lise de Caracter√≠sticas
        - As 3 caracter√≠sticas mais importantes s√£o: {}
        - Caracter√≠sticas com menor impacto: {}
        - Recomenda√ß√£o: considerar remover caracter√≠sticas com import√¢ncia < 5%
        """.format(
            ", ".join(feature_importance['Caracter√≠stica'].tail(3).tolist()),
            ", ".join(feature_importance['Caracter√≠stica'].head(2).tolist())
        ))

        st.divider()

        st.subheader("üì• Download dos Resultados")

        results_dict = {
            'M√©trica': ['Precis√£o', 'Pontua√ß√£o F1', 'M√©dia da Valida√ß√£o Cruzada'],
            '√Årvore de Decis√£o': [dt_accuracy, dt_f1, cv_scores.mean()],
            'KNN': [knn_accuracy, knn_f1, cv_scores.mean()]
        }
        results_df = pd.DataFrame(results_dict)

        st.download_button(
            "Download Resultados",
            data=results_df.to_csv(index=False).encode('utf-8'),
            file_name='resultados.csv',
            mime='text/csv'
        )

        st.divider()